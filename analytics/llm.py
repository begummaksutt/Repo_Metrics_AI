"""
LLM Entegrasyon ModÃ¼lÃ¼

Bu modÃ¼l, kalite metriklerini doÄŸal dil aÃ§Ä±klamalarÄ±na dÃ¶nÃ¼ÅŸtÃ¼rÃ¼r.
Desteklenen LLM Provider'lar:
- OpenAI (GPT-4, GPT-3.5)
- Google Gemini
- Anthropic Claude
- Ollama (Yerel LLM)

KullanÄ±m:
    from analytics.llm import generate_quality_report, LLMClient
    
    client = LLMClient(provider="openai", api_key="sk-xxx")
    report = generate_quality_report(analysis_result, client)
"""

import os
import json
from typing import Any, Optional, Literal
from dataclasses import dataclass, field
from abc import ABC, abstractmethod
from datetime import datetime


# Provider tÃ¼rleri
LLMProvider = Literal["openai", "gemini", "claude", "ollama", "mock"]


@dataclass
class LLMConfig:
    """LLM yapÄ±landÄ±rmasÄ±."""
    provider: LLMProvider = "openai"
    model: str = "gpt-3.5-turbo"
    api_key: Optional[str] = None
    base_url: Optional[str] = None  # Ollama iÃ§in
    temperature: float = 0.7
    max_tokens: int = 1500
    language: str = "tr"  # Rapor dili


@dataclass
class LLMResponse:
    """LLM yanÄ±t objesi."""
    content: str
    provider: str
    model: str
    tokens_used: int = 0
    success: bool = True
    error: Optional[str] = None


class BaseLLMProvider(ABC):
    """Temel LLM provider sÄ±nÄ±fÄ±."""
    
    @abstractmethod
    def generate(self, prompt: str, system_prompt: str = "") -> LLMResponse:
        """Metin Ã¼retir."""
        pass


class OpenAIProvider(BaseLLMProvider):
    """OpenAI GPT provider."""
    
    def __init__(self, config: LLMConfig):
        self.config = config
        self.api_key = config.api_key or os.getenv("OPENAI_API_KEY")
        
        if not self.api_key:
            raise ValueError("OpenAI API key gerekli. OPENAI_API_KEY env variable veya api_key parametresi kullanÄ±n.")
    
    def generate(self, prompt: str, system_prompt: str = "") -> LLMResponse:
        try:
            import openai
            
            client = openai.OpenAI(api_key=self.api_key)
            
            messages = []
            if system_prompt:
                messages.append({"role": "system", "content": system_prompt})
            messages.append({"role": "user", "content": prompt})
            
            response = client.chat.completions.create(
                model=self.config.model,
                messages=messages,
                temperature=self.config.temperature,
                max_tokens=self.config.max_tokens
            )
            
            return LLMResponse(
                content=response.choices[0].message.content,
                provider="openai",
                model=self.config.model,
                tokens_used=response.usage.total_tokens if response.usage else 0
            )
            
        except ImportError:
            return LLMResponse(
                content="",
                provider="openai",
                model=self.config.model,
                success=False,
                error="openai paketi yÃ¼klÃ¼ deÄŸil. 'pip install openai' komutunu Ã§alÄ±ÅŸtÄ±rÄ±n."
            )
        except Exception as e:
            return LLMResponse(
                content="",
                provider="openai",
                model=self.config.model,
                success=False,
                error=str(e)
            )


class GeminiProvider(BaseLLMProvider):
    """Google Gemini provider."""
    
    def __init__(self, config: LLMConfig):
        self.config = config
        self.api_key = config.api_key or os.getenv("GOOGLE_API_KEY")
        
        if not self.api_key:
            raise ValueError("Google API key gerekli. GOOGLE_API_KEY env variable veya api_key parametresi kullanÄ±n.")
    
    def generate(self, prompt: str, system_prompt: str = "") -> LLMResponse:
        try:
            import google.generativeai as genai
            
            genai.configure(api_key=self.api_key)
            model = genai.GenerativeModel(self.config.model or "gemini-pro")
            
            full_prompt = f"{system_prompt}\n\n{prompt}" if system_prompt else prompt
            
            response = model.generate_content(
                full_prompt,
                generation_config=genai.types.GenerationConfig(
                    temperature=self.config.temperature,
                    max_output_tokens=self.config.max_tokens
                )
            )
            
            return LLMResponse(
                content=response.text,
                provider="gemini",
                model=self.config.model or "gemini-pro"
            )
            
        except ImportError:
            return LLMResponse(
                content="",
                provider="gemini",
                model=self.config.model,
                success=False,
                error="google-generativeai paketi yÃ¼klÃ¼ deÄŸil. 'pip install google-generativeai' komutunu Ã§alÄ±ÅŸtÄ±rÄ±n."
            )
        except Exception as e:
            return LLMResponse(
                content="",
                provider="gemini",
                model=self.config.model,
                success=False,
                error=str(e)
            )


class ClaudeProvider(BaseLLMProvider):
    """Anthropic Claude provider."""
    
    def __init__(self, config: LLMConfig):
        self.config = config
        self.api_key = config.api_key or os.getenv("ANTHROPIC_API_KEY")
        
        if not self.api_key:
            raise ValueError("Anthropic API key gerekli. ANTHROPIC_API_KEY env variable veya api_key parametresi kullanÄ±n.")
    
    def generate(self, prompt: str, system_prompt: str = "") -> LLMResponse:
        try:
            import anthropic
            
            client = anthropic.Anthropic(api_key=self.api_key)
            
            message = client.messages.create(
                model=self.config.model or "claude-3-haiku-20240307",
                max_tokens=self.config.max_tokens,
                system=system_prompt if system_prompt else "Sen bir yazÄ±lÄ±m kalite analiz uzmanÄ±sÄ±n.",
                messages=[{"role": "user", "content": prompt}]
            )
            
            return LLMResponse(
                content=message.content[0].text,
                provider="claude",
                model=self.config.model or "claude-3-haiku-20240307",
                tokens_used=message.usage.input_tokens + message.usage.output_tokens
            )
            
        except ImportError:
            return LLMResponse(
                content="",
                provider="claude",
                model=self.config.model,
                success=False,
                error="anthropic paketi yÃ¼klÃ¼ deÄŸil. 'pip install anthropic' komutunu Ã§alÄ±ÅŸtÄ±rÄ±n."
            )
        except Exception as e:
            return LLMResponse(
                content="",
                provider="claude",
                model=self.config.model,
                success=False,
                error=str(e)
            )


class OllamaProvider(BaseLLMProvider):
    """Ollama yerel LLM provider."""
    
    def __init__(self, config: LLMConfig):
        self.config = config
        self.base_url = config.base_url or "http://localhost:11434"
    
    def generate(self, prompt: str, system_prompt: str = "") -> LLMResponse:
        try:
            import requests
            
            full_prompt = f"{system_prompt}\n\n{prompt}" if system_prompt else prompt
            
            response = requests.post(
                f"{self.base_url}/api/generate",
                json={
                    "model": self.config.model or "llama2",
                    "prompt": full_prompt,
                    "stream": False,
                    "options": {
                        "temperature": self.config.temperature,
                        "num_predict": self.config.max_tokens
                    }
                },
                timeout=120
            )
            
            if response.status_code == 200:
                data = response.json()
                return LLMResponse(
                    content=data.get("response", ""),
                    provider="ollama",
                    model=self.config.model or "llama2"
                )
            else:
                return LLMResponse(
                    content="",
                    provider="ollama",
                    model=self.config.model,
                    success=False,
                    error=f"Ollama hatasÄ±: {response.status_code}"
                )
                
        except Exception as e:
            return LLMResponse(
                content="",
                provider="ollama",
                model=self.config.model,
                success=False,
                error=f"Ollama baÄŸlantÄ± hatasÄ±: {str(e)}"
            )


class MockProvider(BaseLLMProvider):
    """Test amaÃ§lÄ± mock provider - LLM olmadan Ã§alÄ±ÅŸÄ±r."""
    
    def __init__(self, config: LLMConfig):
        self.config = config
    
    def generate(self, prompt: str, system_prompt: str = "") -> LLMResponse:
        # Basit kural tabanlÄ± yanÄ±t Ã¼ret
        return LLMResponse(
            content=self._generate_mock_response(prompt),
            provider="mock",
            model="rule-based"
        )
    
    def _generate_mock_response(self, prompt: str) -> str:
        """Basit kural tabanlÄ± yanÄ±t Ã¼retir."""
        # Prompt'tan metrikleri Ã§Ä±karmaya Ã§alÄ±ÅŸ
        lines = []
        
        if "genel skor" in prompt.lower() or "overall" in prompt.lower():
            lines.append("## ğŸ“Š Genel DeÄŸerlendirme\n")
            lines.append("Bu repository, yazÄ±lÄ±m kalite standartlarÄ± aÃ§Ä±sÄ±ndan deÄŸerlendirilmiÅŸtir.")
        
        if "commit" in prompt.lower():
            lines.append("\n### ğŸ“ Commit Analizi")
            lines.append("Commit sÄ±klÄ±ÄŸÄ± proje aktivitesini gÃ¶stermektedir. ")
            lines.append("DÃ¼zenli commit'ler, aktif geliÅŸtirme sÃ¼recinin bir gÃ¶stergesidir.")
        
        if "test" in prompt.lower():
            lines.append("\n### ğŸ§ª Test Durumu")
            lines.append("Test coverage oranÄ±, kod kalitesinin Ã¶nemli bir gÃ¶stergesidir. ")
            lines.append("YÃ¼ksek test oranÄ±, gÃ¼venilir bir kod tabanÄ± anlamÄ±na gelir.")
        
        if "issue" in prompt.lower():
            lines.append("\n### ğŸ› Issue YÃ¶netimi")
            lines.append("Issue Ã§Ã¶zÃ¼m sÃ¼resi, ekip verimliliÄŸini yansÄ±tÄ±r. ")
            lines.append("HÄ±zlÄ± issue Ã§Ã¶zÃ¼mÃ¼, iyi bir proje yÃ¶netiminin iÅŸaretidir.")
        
        if "pr" in prompt.lower() or "pull request" in prompt.lower():
            lines.append("\n### ğŸ”€ Pull Request Kalitesi")
            lines.append("PR kabul oranÄ±, kod review sÃ¼recinin etkinliÄŸini gÃ¶sterir. ")
            lines.append("DÃ¼ÅŸÃ¼k red oranÄ±, kaliteli kod submission'larÄ±na iÅŸaret eder.")
        
        lines.append("\n---")
        lines.append("*Bu rapor otomatik olarak oluÅŸturulmuÅŸtur.*")
        
        return "\n".join(lines)


class LLMClient:
    """
    BirleÅŸik LLM Client.
    
    FarklÄ± provider'larÄ± tek bir arayÃ¼z Ã¼zerinden kullanmayÄ± saÄŸlar.
    
    KullanÄ±m:
        # OpenAI
        client = LLMClient(provider="openai", api_key="sk-xxx")
        
        # Gemini
        client = LLMClient(provider="gemini", api_key="xxx")
        
        # Yerel Ollama
        client = LLMClient(provider="ollama", model="llama2")
        
        # Mock (test iÃ§in)
        client = LLMClient(provider="mock")
    """
    
    PROVIDERS = {
        "openai": OpenAIProvider,
        "gemini": GeminiProvider,
        "claude": ClaudeProvider,
        "ollama": OllamaProvider,
        "mock": MockProvider
    }
    
    def __init__(
        self,
        provider: LLMProvider = "mock",
        api_key: Optional[str] = None,
        model: Optional[str] = None,
        **kwargs
    ):
        """
        LLMClient oluÅŸturur.
        
        Args:
            provider: LLM provider ("openai", "gemini", "claude", "ollama", "mock")
            api_key: API anahtarÄ±
            model: Model adÄ±
            **kwargs: Ek yapÄ±landÄ±rma parametreleri
        """
        # VarsayÄ±lan modeller
        default_models = {
            "openai": "gpt-3.5-turbo",
            "gemini": "gemini-pro",
            "claude": "claude-3-haiku-20240307",
            "ollama": "llama2",
            "mock": "rule-based"
        }
        
        self.config = LLMConfig(
            provider=provider,
            api_key=api_key,
            model=model or default_models.get(provider, ""),
            **kwargs
        )
        
        provider_class = self.PROVIDERS.get(provider)
        if not provider_class:
            raise ValueError(f"Desteklenmeyen provider: {provider}")
        
        self.provider = provider_class(self.config)
    
    def generate(self, prompt: str, system_prompt: str = "") -> LLMResponse:
        """Metin Ã¼retir."""
        return self.provider.generate(prompt, system_prompt)
    
    def generate_report(self, analysis: dict[str, Any]) -> LLMResponse:
        """Analiz sonuÃ§larÄ±ndan rapor Ã¼retir."""
        prompt = _build_analysis_prompt(analysis, self.config.language)
        system_prompt = _get_system_prompt(self.config.language)
        return self.generate(prompt, system_prompt)


def _get_system_prompt(language: str = "tr") -> str:
    """Sistem prompt'u dÃ¶ndÃ¼rÃ¼r."""
    if language == "tr":
        return """Sen deneyimli bir yazÄ±lÄ±m kalite gÃ¼vence uzmanÄ±sÄ±n. 
GitHub repository metriklerini analiz edip, anlaÅŸÄ±lÄ±r ve actionable raporlar Ã¼retiyorsun.

RaporlarÄ±nda:
- Teknik terimleri aÃ§Ä±k bir dille anlat
- GÃ¼Ã§lÃ¼ ve zayÄ±f yÃ¶nleri belirt
- Somut iyileÅŸtirme Ã¶nerileri sun
- Profesyonel ama samimi bir ton kullan
- Emoji kullanarak raporlarÄ± gÃ¶rsel olarak zenginleÅŸtir"""
    else:
        return """You are an experienced software quality assurance expert.
You analyze GitHub repository metrics and produce clear, actionable reports.

In your reports:
- Explain technical terms in plain language
- Highlight strengths and weaknesses
- Provide concrete improvement suggestions
- Use a professional but friendly tone
- Use emojis to visually enrich reports"""


def _build_analysis_prompt(analysis: dict[str, Any], language: str = "tr") -> str:
    """Analiz verilerinden prompt oluÅŸturur."""
    
    if not analysis.get("success"):
        return "Analiz baÅŸarÄ±sÄ±z oldu, rapor Ã¼retilemedi."
    
    repo = analysis.get("repository", {})
    metrics = analysis.get("metrics", {})
    trends = analysis.get("trends", {})
    overall = analysis.get("overall", {})
    stats = analysis.get("stats", {})
    
    # Metrik detaylarÄ±
    commit_freq = metrics.get("commit_frequency", {})
    issue_res = metrics.get("issue_resolution", {})
    pr_rej = metrics.get("pr_rejection", {})
    test_ratio = metrics.get("test_ratio", {})
    
    # Trend detaylarÄ±
    commit_trend = trends.get("commit_trend", {})
    issue_trend = trends.get("issue_trend", {})
    
    if language == "tr":
        prompt = f"""AÅŸaÄŸÄ±daki GitHub repository analiz sonuÃ§larÄ±nÄ± deÄŸerlendirip detaylÄ± bir kalite raporu oluÅŸtur:

## Repository Bilgileri
- **Ad:** {repo.get('full_name', 'Bilinmiyor')}
- **AÃ§Ä±klama:** {repo.get('description', 'AÃ§Ä±klama yok')}
- **Ana Dil:** {repo.get('language', 'Bilinmiyor')}
- **Stars:** {repo.get('stars', 0):,}
- **Forks:** {repo.get('forks', 0):,}

## Genel Skor
- **Puan:** {overall.get('overall_score', 0):.1f}/100
- **Not:** {overall.get('grade', 'N/A')}

## Metrik DetaylarÄ±

### 1. Commit SÄ±klÄ±ÄŸÄ±
- GÃ¼nlÃ¼k ortalama: {commit_freq.get('raw', 0):.2f} commit
- Skor: {commit_freq.get('score', 0):.0f}/100
- Toplam commit (son 90 gÃ¼n): {stats.get('total_commits', 0)}
- Trend: {commit_trend.get('trend_direction', 'bilinmiyor')} ({commit_trend.get('trend_strength', 'belirsiz')})

### 2. Issue Ã‡Ã¶zÃ¼m SÃ¼resi
- Ortalama Ã§Ã¶zÃ¼m: {issue_res.get('raw', 0):.1f} gÃ¼n
- Skor: {issue_res.get('score', 0):.0f}/100
- Ã‡Ã¶zÃ¼len issue sayÄ±sÄ±: {issue_res.get('resolved_count', 0)}
- Trend: {issue_trend.get('trend_direction', 'bilinmiyor')}

### 3. PR Kalitesi
- Red oranÄ±: %{pr_rej.get('raw', 0)*100:.1f}
- Skor: {pr_rej.get('score', 0):.0f}/100
- Merge edilen: {pr_rej.get('merged', 0)}
- Reddedilen: {pr_rej.get('rejected', 0)}

### 4. Test Coverage
- Test dosyasÄ± oranÄ±: %{test_ratio.get('raw', 0)*100:.1f}
- Skor: {test_ratio.get('score', 0):.0f}/100
- Test dosyasÄ± sayÄ±sÄ±: {test_ratio.get('test_files', 0)}
- Toplam kod dosyasÄ±: {test_ratio.get('total_files', 0)}

---

Bu verilere dayanarak:
1. Projenin genel durumunu Ã¶zetle
2. En gÃ¼Ã§lÃ¼ 2-3 yÃ¶nÃ¼nÃ¼ belirt
3. Ä°yileÅŸtirme gereken 2-3 alanÄ± tespit et
4. Her alan iÃ§in somut Ã¶neriler sun
5. SonuÃ§ olarak projenin potansiyelini deÄŸerlendir

Raporu Markdown formatÄ±nda, baÅŸlÄ±klar ve bullet point'ler kullanarak oluÅŸtur."""

    else:
        prompt = f"""Evaluate the following GitHub repository analysis results and create a detailed quality report:

## Repository Information
- **Name:** {repo.get('full_name', 'Unknown')}
- **Description:** {repo.get('description', 'No description')}
- **Main Language:** {repo.get('language', 'Unknown')}
- **Stars:** {repo.get('stars', 0):,}
- **Forks:** {repo.get('forks', 0):,}

## Overall Score
- **Score:** {overall.get('overall_score', 0):.1f}/100
- **Grade:** {overall.get('grade', 'N/A')}

## Metric Details

### 1. Commit Frequency
- Daily average: {commit_freq.get('raw', 0):.2f} commits
- Score: {commit_freq.get('score', 0):.0f}/100
- Total commits (last 90 days): {stats.get('total_commits', 0)}
- Trend: {commit_trend.get('trend_direction', 'unknown')} ({commit_trend.get('trend_strength', 'uncertain')})

### 2. Issue Resolution Time
- Average resolution: {issue_res.get('raw', 0):.1f} days
- Score: {issue_res.get('score', 0):.0f}/100
- Resolved issues: {issue_res.get('resolved_count', 0)}
- Trend: {issue_trend.get('trend_direction', 'unknown')}

### 3. PR Quality
- Rejection rate: {pr_rej.get('raw', 0)*100:.1f}%
- Score: {pr_rej.get('score', 0):.0f}/100
- Merged: {pr_rej.get('merged', 0)}
- Rejected: {pr_rej.get('rejected', 0)}

### 4. Test Coverage
- Test file ratio: {test_ratio.get('raw', 0)*100:.1f}%
- Score: {test_ratio.get('score', 0):.0f}/100
- Test files: {test_ratio.get('test_files', 0)}
- Total code files: {test_ratio.get('total_files', 0)}

---

Based on this data:
1. Summarize the overall project status
2. Identify the 2-3 strongest aspects
3. Identify 2-3 areas needing improvement
4. Provide concrete suggestions for each area
5. Evaluate the project's potential

Create the report in Markdown format using headings and bullet points."""

    return prompt


def generate_quality_report(
    analysis: dict[str, Any],
    client: Optional[LLMClient] = None,
    provider: LLMProvider = "mock",
    api_key: Optional[str] = None,
    language: str = "tr"
) -> dict[str, Any]:
    """
    Analiz sonuÃ§larÄ±ndan LLM destekli kalite raporu Ã¼retir.
    
    Args:
        analysis: analyze_repository() Ã§Ä±ktÄ±sÄ±
        client: Mevcut LLMClient (opsiyonel)
        provider: LLM provider (client yoksa kullanÄ±lÄ±r)
        api_key: API anahtarÄ± (client yoksa kullanÄ±lÄ±r)
        language: Rapor dili ("tr" veya "en")
        
    Returns:
        {
            "success": bool,
            "report": str (Markdown formatÄ±nda rapor),
            "provider": str,
            "model": str,
            "tokens_used": int,
            "error": str | None
        }
    """
    if not analysis.get("success"):
        return {
            "success": False,
            "report": "",
            "provider": "",
            "model": "",
            "tokens_used": 0,
            "error": f"Analiz baÅŸarÄ±sÄ±z: {analysis.get('error', 'Bilinmeyen hata')}"
        }
    
    # Client oluÅŸtur veya mevcut olanÄ± kullan
    if not client:
        try:
            client = LLMClient(provider=provider, api_key=api_key, language=language)
        except ValueError as e:
            return {
                "success": False,
                "report": "",
                "provider": provider,
                "model": "",
                "tokens_used": 0,
                "error": str(e)
            }
    
    # Rapor Ã¼ret
    response = client.generate_report(analysis)
    
    if not response.success:
        return {
            "success": False,
            "report": "",
            "provider": response.provider,
            "model": response.model,
            "tokens_used": 0,
            "error": response.error
        }
    
    return {
        "success": True,
        "report": response.content,
        "provider": response.provider,
        "model": response.model,
        "tokens_used": response.tokens_used,
        "error": None
    }


def generate_metric_explanation(
    metric_name: str,
    metric_data: dict[str, Any],
    client: Optional[LLMClient] = None,
    language: str = "tr"
) -> str:
    """
    Tek bir metrik iÃ§in kÄ±sa aÃ§Ä±klama Ã¼retir.
    
    Args:
        metric_name: Metrik adÄ± (commit_frequency, issue_resolution, vb.)
        metric_data: Metrik verisi {"raw": ..., "score": ...}
        client: LLMClient instance
        language: Dil
        
    Returns:
        AÃ§Ä±klama metni
    """
    if not client:
        client = LLMClient(provider="mock", language=language)
    
    score = metric_data.get("score", 0)
    raw = metric_data.get("raw", 0)
    
    metric_names_tr = {
        "commit_frequency": "Commit SÄ±klÄ±ÄŸÄ±",
        "issue_resolution": "Issue Ã‡Ã¶zÃ¼m SÃ¼resi",
        "pr_rejection": "PR Kalitesi",
        "test_ratio": "Test Coverage"
    }
    
    metric_name_display = metric_names_tr.get(metric_name, metric_name)
    
    if language == "tr":
        prompt = f"""'{metric_name_display}' metriÄŸi iÃ§in kÄ±sa (2-3 cÃ¼mle) bir deÄŸerlendirme yaz:
- Skor: {score:.0f}/100
- Ham deÄŸer: {raw}

DeÄŸerlendirme pozitif veya negatif olsun, skora gÃ¶re. Somut ve anlaÅŸÄ±lÄ±r ol."""
    else:
        prompt = f"""Write a brief (2-3 sentences) evaluation for the '{metric_name}' metric:
- Score: {score:.0f}/100
- Raw value: {raw}

The evaluation should be positive or negative based on the score. Be concrete and clear."""
    
    response = client.generate(prompt)
    return response.content if response.success else f"Skor: {score:.0f}/100"


def generate_improvement_suggestions(
    analysis: dict[str, Any],
    client: Optional[LLMClient] = None,
    language: str = "tr"
) -> list[dict[str, str]]:
    """
    Ä°yileÅŸtirme Ã¶nerileri Ã¼retir.
    
    Args:
        analysis: analyze_repository() Ã§Ä±ktÄ±sÄ±
        client: LLMClient instance
        language: Dil
        
    Returns:
        [{"area": "...", "suggestion": "...", "priority": "high/medium/low"}, ...]
    """
    if not client:
        client = LLMClient(provider="mock", language=language)
    
    metrics = analysis.get("metrics", {})
    
    # En dÃ¼ÅŸÃ¼k skorlu metrikleri bul
    metric_scores = []
    for name, data in metrics.items():
        if isinstance(data, dict) and "score" in data:
            metric_scores.append((name, data.get("score", 0)))
    
    metric_scores.sort(key=lambda x: x[1])
    
    suggestions = []
    
    # DÃ¼ÅŸÃ¼k skorlu metrikler iÃ§in Ã¶neri oluÅŸtur
    priority_map = {0: "high", 1: "high", 2: "medium", 3: "low"}
    
    suggestion_templates_tr = {
        "commit_frequency": {
            "area": "Commit SÄ±klÄ±ÄŸÄ±",
            "low": "Daha sÄ±k ve kÃ¼Ã§Ã¼k commit'ler yapÄ±n. Atomic commit prensibi uygulayÄ±n.",
            "medium": "Commit sÄ±klÄ±ÄŸÄ±nÄ± artÄ±rÄ±n. GÃ¼nlÃ¼k en az 1-2 commit hedefleyin.",
            "high": "Mevcut commit sÄ±klÄ±ÄŸÄ±nÄ±z iyi. Kaliteyi koruyun."
        },
        "issue_resolution": {
            "area": "Issue YÃ¶netimi",
            "low": "Issue'larÄ± Ã¶nceliklendirin ve SLA tanÄ±mlayÄ±n. Sprint planlamasÄ± yapÄ±n.",
            "medium": "Issue Ã§Ã¶zÃ¼m sÃ¼resini kÄ±saltmak iÃ§in triage sÃ¼reci oluÅŸturun.",
            "high": "Issue yÃ¶netiminiz baÅŸarÄ±lÄ±. Best practice'leri dokÃ¼mante edin."
        },
        "pr_rejection": {
            "area": "PR Kalitesi",
            "low": "PR ÅŸablonu oluÅŸturun. Code review checklist'i tanÄ±mlayÄ±n.",
            "medium": "PR aÃ§madan Ã¶nce self-review yapÄ±n. Test coverage'Ä± kontrol edin.",
            "high": "PR kalitesi yÃ¼ksek. Pair programming ile daha da geliÅŸtirin."
        },
        "test_ratio": {
            "area": "Test Coverage",
            "low": "Unit test eklemeye baÅŸlayÄ±n. Kritik fonksiyonlarÄ± Ã¶nceliklendirin.",
            "medium": "Test coverage'Ä± artÄ±rÄ±n. CI/CD'ye test gate ekleyin.",
            "high": "Test coverage iyi. Integration ve E2E testleri deÄŸerlendirin."
        }
    }
    
    for i, (metric_name, score) in enumerate(metric_scores):
        templates = suggestion_templates_tr.get(metric_name, {})
        
        if score < 40:
            level = "low"
        elif score < 70:
            level = "medium"
        else:
            level = "high"
        
        suggestions.append({
            "area": templates.get("area", metric_name),
            "suggestion": templates.get(level, "Ä°yileÅŸtirme Ã¶nerisi mevcut deÄŸil."),
            "priority": priority_map.get(i, "low"),
            "current_score": score
        })
    
    return suggestions


# CLI ve test
if __name__ == "__main__":
    # Mock client ile test
    print("ğŸ¤– LLM ModÃ¼lÃ¼ Test\n")
    
    # Ã–rnek analiz verisi
    sample_analysis = {
        "success": True,
        "repository": {
            "full_name": "test/repo",
            "description": "Test repository",
            "language": "Python",
            "stars": 100,
            "forks": 25
        },
        "metrics": {
            "commit_frequency": {"raw": 2.5, "score": 65},
            "issue_resolution": {"raw": 5.2, "score": 45},
            "pr_rejection": {"raw": 0.15, "score": 70},
            "test_ratio": {"raw": 0.18, "score": 35}
        },
        "trends": {
            "commit_trend": {"trend_direction": "artan", "trend_strength": "orta"},
            "issue_trend": {"trend_direction": "iyileÅŸiyor"}
        },
        "overall": {
            "overall_score": 54,
            "grade": "C+"
        },
        "stats": {
            "total_commits": 150,
            "total_issues": 45
        }
    }
    
    # Mock provider ile rapor Ã¼ret
    result = generate_quality_report(sample_analysis, provider="mock")
    
    if result["success"]:
        print("âœ… Rapor Ã¼retildi:\n")
        print(result["report"])
        print(f"\nProvider: {result['provider']}")
    else:
        print(f"âŒ Hata: {result['error']}")
    
    # Ä°yileÅŸtirme Ã¶nerileri
    print("\n" + "="*50)
    print("ğŸ“‹ Ä°yileÅŸtirme Ã–nerileri:\n")
    
    suggestions = generate_improvement_suggestions(sample_analysis)
    for s in suggestions:
        priority_emoji = {"high": "ğŸ”´", "medium": "ğŸŸ¡", "low": "ğŸŸ¢"}.get(s["priority"], "âšª")
        print(f"{priority_emoji} **{s['area']}** (Skor: {s['current_score']:.0f})")
        print(f"   {s['suggestion']}\n")

